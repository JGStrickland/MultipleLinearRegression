{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Multiple Feature Linear Regression\n",
    "\n",
    "---\n",
    "\n",
    "Author: JGStrickland\n",
    "\n",
    "Date: June 2025\n",
    "\n",
    "Description: This notebook demonstrates how to build a linear regression model from scratch using Python and Numpy, without relying on machine learning libraries.\n",
    "\n",
    "It covers the following steps:\n",
    "1. Class Definition – Creating a reusable Linear Regression class for multiple features.\n",
    "2. Fitting the Model – Using the Normal Equation to calculate the optimal slopes and intercept.\n",
    "3. Making Predictions – Predicting new data points with the trained model.\n",
    "4. Evaluation – Measuring model performance using MAE, MSE, and R² score.\n",
    "5. Testing - Testing model performance on real world datasets.\n",
    "\n",
    "This notebook is intended as a hands-on introduction to linear regression and demonstrates how the underlying mathematics can be implemented programmatically.\n"
   ],
   "id": "8d7c5a5bc1fbbbdb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step Zero - Imports\n",
    "\n",
    "---\n",
    "We will use Numpy to handle the matrix multiplication needed in this project. Numpy is preferred over native Python lists because it is much faster, more memory-efficient, and provides built-in support for linear algebra operations like matrix multiplication, transposes, and inverses."
   ],
   "id": "2029ed73745ede1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:10:20.572420Z",
     "start_time": "2025-08-25T16:10:20.568947Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "23cdb5616ebff0fe",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step One - Creating the class\n",
    "\n",
    "---\n",
    "\n",
    "First, we need to create a class for the Linear Regression Model so we can have multiple instances of our model to test across different data sets. Here we will call it LinearRegressor. This class will contain three attributes:\n",
    "\n",
    "- coef_ => this is the coefficient or the slope(s) for features\n",
    "- intercept_ => this is our y intercept or the bias term\n",
    "- fitted => this is a boolean value to keep trac wether the model is fitted to the data or not, this starts off as false as our model is not immediately fitted to data"
   ],
   "id": "6c7b276dd34d6886"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:10:20.583980Z",
     "start_time": "2025-08-25T16:10:20.581013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LinearRegressionMulti:\n",
    "    def __init__(self):\n",
    "        self.coef_ = None       # slope(s) for features\n",
    "        self.intercept_ = None  # bias term\n",
    "        self.fitted = False"
   ],
   "id": "2834b250f3740fb7",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step Two - Fitting the data\n",
    "\n",
    "---\n",
    "\n",
    "### Understanding the concept of linear regression\n",
    "\n",
    "In linear regression, we try to fit a straight line through our data points that best predicts the target $y$ from one or more features $X$. The line is generally written as:\n",
    "\n",
    "\n",
    "$$\n",
    "y = mX + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}$ is the predicted value of y.\n",
    "- $m$(slope) determines how much the prediction changes for a unit change in the feature.\n",
    "- - Example: if $m = 2$, then for every 1 unit increase in $X$, $\\hat{y}$ increases by 2.\n",
    "- $b$ (intercept or bias) determined where the line crosses the $y$-axis.\n",
    "- - The intercept is the starting point of the line when all features are zero. It shifts the line up or down so that the model can best fit the data instead of being forced through the origin.\n",
    "\n",
    "for multiple features, this generalises to:\n",
    "$$\n",
    "\\hat{y} = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + ... \\beta_{n}X_{n}\n",
    "$$\n",
    "where:\n",
    "- $\\beta_{0}$ = intercept/bias.\n",
    "- $\\beta_{1},...\\beta_{n}$ = slopes for each fetaure.\n",
    "\n",
    "The goal of linear regression is to find the slopes and intercept the makes the predictions $\\hat{y}$ as close as possible to the actual values of $y$.\n",
    "\n",
    "---\n",
    "\n",
    "### Cost function: Measuring error\n",
    "\n",
    "To measure how accurate/ far off our predictions are we define a cost/loss function $J\\left(\\beta\\right) as the sum of squared errors:\n",
    "\n",
    "$$\n",
    "J\\left(\\beta\\right) = \\left\\|y - \\hat{y} \\right\\|^{2}\n",
    "$$\n",
    "\n",
    "In matrix form where $\\hat{y} = X\\beta$:\n",
    "\n",
    "$$\n",
    "J\\left(\\beta\\right) = \\left(y - X\\beta\\right)^{T} \\left(y - X\\beta\\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "### Adding bias\n",
    "\n",
    "Instead of calculating both the slopes and intercept seperately we can add a column of ones to $X$ to represent the bias. This lets the model learn the intercept and the slopes all at once. This helps the model generalise to any number of features.\n",
    "\n",
    "---\n",
    "\n",
    "### Solving the parameters: The Normal Equation\n",
    "\n",
    "To find the best $\\beta$ that minimises $J\\left(\\beta\\right)$, we use the Normal Equation:\n",
    "\n",
    "$$\n",
    "\\beta = \\left(X^{T}X\\right)^{-1} X^{T}y\n",
    "$$\n",
    "\n",
    "- This gives all slopes and the intercept in a single computation.\n",
    "- The slopes tell us how strongly each feature affects the prediction.\n",
    "- The intercept allows the line to shift up or down so it fits the data properly.\n",
    "\n",
    "Using the Normal Equation, we find the combination of parameters that minimizes the squared error, giving the best linear approximation of the data."
   ],
   "id": "f8a359d6f8126ee6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:10:20.598754Z",
     "start_time": "2025-08-25T16:10:20.596424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LinearRegressionMulti:\n",
    "    def __init__(self):\n",
    "        self.coef_ = None       # slope(s) for features\n",
    "        self.intercept_ = None  # bias term\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit model using Normal Equation\"\"\"\n",
    "        # Convert to numpy arrays\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        # Ensure y is a column vector\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "\n",
    "        # Add bias (column of 1s) to X\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "        # Normal Equation: (XᵀX)^(-1) Xᵀy\n",
    "        theta = np.linalg.pinv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "\n",
    "        # Extract intercept and coefficients\n",
    "        self.intercept_ = theta[0][0]\n",
    "        self.coef_ = theta[1:].flatten()\n",
    "        self.fitted = True"
   ],
   "id": "7f67dc75044cf34",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step Three - Making the Predictions\n",
    "\n",
    "---\n",
    "\n",
    "The predict function uses the trained model parameters (slopes and intercept) to calculate predictions for new data. It takes the feature matrix X, applies the linear equation $\\hat{y} = X\\beta$, and returns the predicted values. If X has only one feature, it automatically reshapes it into a column vector so that the matrix multiplication works correctly.\n"
   ],
   "id": "51d0199bdacaa65f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:10:20.612185Z",
     "start_time": "2025-08-25T16:10:20.609293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LinearRegressionMulti:\n",
    "    def __init__(self):\n",
    "        self.coef_ = None       # slope(s) for features\n",
    "        self.intercept_ = None  # bias term\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit model using Normal Equation\"\"\"\n",
    "        # Convert to numpy arrays\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        # Ensure y is a column vector\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "\n",
    "        # Add bias (column of 1s) to X\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "        # Normal Equation: (XᵀX)^(-1) Xᵀy\n",
    "        theta = np.linalg.pinv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "\n",
    "        # Extract intercept and coefficients\n",
    "        self.intercept_ = theta[0][0]\n",
    "        self.coef_ = theta[1:].flatten()\n",
    "        self.fitted = True\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict values given new data\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Model not fitted yet. Call fit() first.\")\n",
    "\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        # If X is 1D (single feature), reshape it\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "\n",
    "        return X.dot(self.coef_) + self.intercept_"
   ],
   "id": "3c0fb2f8a380d91f",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step Four - Evaluating our Predictions\n",
    "\n",
    "---\n",
    "\n",
    "Once our model is trained, we need a way to quantify how well it is performing. Evaluating the model is crucial to understand if it is making accurate predictions or if it needs improvement. In linear regression, there are several commonly used metrics:\n",
    "\n",
    "### 1. Mean Absolute Error (MAE)\n",
    "\n",
    "The Mean Absolute Error is the average of the absolute differences between the actual values and the predicted values:\n",
    "\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right|\n",
    "$$\n",
    "\n",
    "- Interpretation: MAE gives an intuitive measure of how far off our predictions are, on average. Lower values indicate better predictive accuracy.\n",
    "- Pros: Simple to understand and not sensitive to outliers as much as MSE.\n",
    "- Cons: It does not penalize large errors as strongly as the Mean Squared Error does.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Mean Squared Error (MSE)\n",
    "\n",
    "The Mean Squared Error is the average of the squared differences between the actual and predicted values:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "- Interpretation: Squaring the errors emphasizes larger errors more than smaller ones, which makes MSE sensitive to outliers.\n",
    "- Pros: Provides a stronger penalty for large errors and is differentiable, which is useful for optimization.\n",
    "- Cons: Harder to interpret because it is in squared units of the target variable.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. R² Score (Coefficient of Determination)\n",
    "\n",
    "The R² score measures the proportion of the variance in the dependent variable that is predictable from the independent variables:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "- Interpretation: R² ranges from 0 to 1, where 1 indicates perfect predictions and 0 indicates that the model does no better than simply predicting the mean of the target variable.\n",
    "- Pros: Provides a normalized measure of how well the model explains the variance in the data.\n",
    "- Cons: Can be misleading for models that overfit or when comparing models across different datasets.\n",
    "\n",
    "---\n",
    "\n",
    "By calculating these metrics, we can objectively assess our model’s performance and identify areas for improvement. Typically, a combination of MAE, MSE, and R² is used to get a complete picture of predictive accuracy and error distribution."
   ],
   "id": "f75adbea9b259fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:10:20.657378Z",
     "start_time": "2025-08-25T16:10:20.654473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LinearRegressionMulti:\n",
    "    def __init__(self):\n",
    "        self.coef_ = None       # slope(s) for features\n",
    "        self.intercept_ = None  # bias term\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit model using Normal Equation\"\"\"\n",
    "        # Convert to numpy arrays\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        # Ensure y is a column vector\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "\n",
    "        # Add bias (column of 1s) to X\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "        # Normal Equation: (XᵀX)^(-1) Xᵀy\n",
    "        theta = np.linalg.pinv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "\n",
    "        # Extract intercept and coefficients\n",
    "        self.intercept_ = theta[0][0]\n",
    "        self.coef_ = theta[1:].flatten()\n",
    "        self.fitted = True\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict values given new data\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Model not fitted yet. Call fit() first.\")\n",
    "\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        # If X is 1D (single feature), reshape it\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "\n",
    "        return X.dot(self.coef_) + self.intercept_\n",
    "\n",
    "    # ---------------- Evaluation Metrics ---------------- #\n",
    "\n",
    "    def mae_score(self, y_true, y_pred):\n",
    "        \"\"\"Mean Absolute Error\"\"\"\n",
    "        return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "    def mse_score(self, y_true, y_pred):\n",
    "        \"\"\"Mean Squared Error\"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    def r2_score(self, y_true, y_pred):\n",
    "        \"\"\"R² score (variance explained)\"\"\"\n",
    "        mean_y = np.mean(y_true)\n",
    "        ss_total = np.sum((y_true - mean_y) ** 2)\n",
    "        ss_residual = np.sum((y_true - y_pred) ** 2)\n",
    "        return 1 - (ss_residual / ss_total) if ss_total != 0 else 0"
   ],
   "id": "af908fc6f79ebccf",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step Five - Testing Our Model\n",
    "\n",
    "---\n",
    "\n",
    "Now that we have a working linear regression model, we can test it on real-world datasets to evaluate its performance. We will use two datasets from `scikit-learn`:\n",
    "\n",
    "1. California Housing Dataset\n",
    "   - Predict median house values based on 8 features like income, population, and housing density.\n",
    "   - A larger dataset with real-world complexity.\n",
    "\n",
    "2. Diabetes Dataset\n",
    "   - Predict a quantitative measure of disease progression after one year using 10 clinical features.\n",
    "   - Smaller and simpler, ideal for testing and debugging.\n",
    "\n",
    "We will:\n",
    "- Load the dataset\n",
    "- Split it into features (`X`) and target (`y`)\n",
    "- Fit our model\n",
    "- Make predictions\n",
    "- Evaluate performance using MAE, MSE, and R² score"
   ],
   "id": "8b7d77901282cf8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:10:20.677107Z",
     "start_time": "2025-08-25T16:10:20.665873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import fetch_california_housing, load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize our Linear Regression model\n",
    "model = LinearRegressionMulti()\n",
    "\n",
    "# ----------------- Test on California Housing ----------------- #\n",
    "# Load dataset\n",
    "california = fetch_california_housing()\n",
    "X_cal, y_cal = california.data, california.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train_cal, X_test_cal, y_train_cal, y_test_cal = train_test_split(X_cal, y_cal, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_cal, y_train_cal)\n",
    "\n",
    "# Predict\n",
    "y_pred_cal = model.predict(X_test_cal)\n",
    "\n",
    "# Evaluate\n",
    "print(\"California Housing Dataset\")\n",
    "print(\"---------------------------\")\n",
    "print(\"MAE:\", model.mae_score(y_test_cal, y_pred_cal))\n",
    "print(\"MSE:\", model.mse_score(y_test_cal, y_pred_cal))\n",
    "print(\"R²:\", model.r2_score(y_test_cal, y_pred_cal))\n",
    "print(\"\\n\")\n",
    "\n",
    "# ----------------- Test on Diabetes Dataset ----------------- #\n",
    "# Load dataset\n",
    "diabetes = load_diabetes()\n",
    "X_dia, y_dia = diabetes.data, diabetes.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train_dia, X_test_dia, y_train_dia, y_test_dia = train_test_split(X_dia, y_dia, test_size=0.2, random_state=36)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_dia, y_train_dia)\n",
    "\n",
    "# Predict\n",
    "y_pred_dia = model.predict(X_test_dia)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Diabetes Dataset\")\n",
    "print(\"----------------\")\n",
    "print(\"MAE:\", model.mae_score(y_test_dia, y_pred_dia))\n",
    "print(\"MSE:\", model.mse_score(y_test_dia, y_pred_dia))\n",
    "print(\"R²:\", model.r2_score(y_test_dia, y_pred_dia))\n"
   ],
   "id": "8589b87c71767ecd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California Housing Dataset\n",
      "---------------------------\n",
      "MAE: 0.5332001305418295\n",
      "MSE: 0.5558915987280281\n",
      "R²: 0.5757877060074328\n",
      "\n",
      "\n",
      "Diabetes Dataset\n",
      "----------------\n",
      "MAE: 44.817857530800275\n",
      "MSE: 3013.5452613146817\n",
      "R²: 0.40748760654298455\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we implemented a Linear Regression model from scratch capable of handling multiple features. This exercise illustrates the core principles of linear regression, including fitting parameters using the Normal Equation, making predictions, and evaluating model performance with metrics such as MAE, MSE, and R².\n",
    "\n",
    "### Strengths of Linear Regression\n",
    "- Simplicity and interpretability: Linear regression is easy to understand and provides direct insight into how each feature affects the target variable.\n",
    "- Efficiency: Using the Normal Equation allows for a closed-form solution without iterative optimization.\n",
    "- Foundation for more complex models: It serves as the basis for many advanced regression and machine learning methods.\n",
    "\n",
    "### Weaknesses of Linear Regression\n",
    "- Limited to linear relationships: It assumes that the relationship between features and target is linear, which may not hold in real-world data.\n",
    "- Sensitive to outliers: Extreme values can disproportionately influence the slope and intercept.\n",
    "- Feature scaling may be necessary: Variables on very different scales can affect model performance, especially in gradient-based variants.\n",
    "\n",
    "### Analysis of Results\n",
    "- California Housing Dataset: The model achieved an R² of approximately 0.58, indicating that it explains over half of the variance in median house prices. While this is a reasonable result for a simple linear model, the moderate R² suggests that some non-linear relationships and complex interactions between features are not captured.\n",
    "- Diabetes Dataset: The R² of around 0.45 reflects a modest fit, highlighting that linear regression can capture general trends but may struggle with more subtle or non-linear effects present in clinical data. The MAE and MSE values confirm that the predictions are reasonably close to actual values but leave room for improvement.\n",
    "\n",
    "Overall, this notebook demonstrates that linear regression is a powerful and interpretable tool for baseline modeling, but its assumptions and limitations must be considered when applying it to real-world datasets. Further improvements could include feature scaling, polynomial features, regularization, or more sophisticated algorithms to better capture complex patterns.\n",
    "\n",
    "\n"
   ],
   "id": "75474d75e52a3d54"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
